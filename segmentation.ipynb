{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235a1715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "import scipy.misc\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "# numpy.set_printoptions(threshold=sys.maxsize)\n",
    "torch.set_printoptions(threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05559eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta1 hyperparam for Adam optimizers\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db73e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_masks(filepath):\n",
    "    '''\n",
    "    Read masks from directory and tranform to categorical\n",
    "    '''\n",
    "    file_list = [file for file in os.listdir(filepath) if file.endswith('.png')]\n",
    "    file_list.sort()\n",
    "    n_masks = len(file_list)\n",
    "    masks = np.empty((n_masks, 512, 512))\n",
    "\n",
    "    for i, file in enumerate(file_list):\n",
    "        mask = image.imread(os.path.join(filepath, file))\n",
    "        mask = (mask >= 128).astype(int)\n",
    "        mask = 4 * mask[:, :, 0] + 2 * mask[:, :, 1] + mask[:, :, 2]\n",
    "        masks[i, mask == 3] = 0  # (Cyan: 011) Urban land \n",
    "        masks[i, mask == 6] = 1  # (Yellow: 110) Agriculture land \n",
    "        masks[i, mask == 5] = 2  # (Purple: 101) Rangeland \n",
    "        masks[i, mask == 2] = 3  # (Green: 010) Forest land \n",
    "        masks[i, mask == 1] = 4  # (Blue: 001) Water \n",
    "        masks[i, mask == 7] = 5  # (White: 111) Barren land \n",
    "        masks[i, mask == 0] = 6  # (Black: 000) Unknown \n",
    "\n",
    "    return masks\n",
    "\n",
    "def mean_iou_score(pred, labels):\n",
    "    '''\n",
    "    Compute mean IoU score over 6 classes\n",
    "    '''\n",
    "    mean_iou = 0\n",
    "    for i in range(6):\n",
    "        tp_fp = np.sum(pred == i)\n",
    "        tp_fn = np.sum(labels == i)\n",
    "        tp = np.sum((pred == i) * (labels == i))\n",
    "        iou = tp / (tp_fp + tp_fn - tp)\n",
    "        mean_iou += iou / 6\n",
    "        print('class #%d : %1.5f'%(i, iou))\n",
    "    print('\\nmean_iou: %f\\n' % mean_iou)\n",
    "\n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80447d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegData(Dataset):\n",
    "    def __init__(self, dir, transform = None):\n",
    "        pattern = \"[0-9]*_mask\"\n",
    "        imgs = []\n",
    "        img_list = os.listdir(dir)\n",
    "        img_list.sort()\n",
    "        n_masks = len(img_list) // 2\n",
    "        masks = np.empty((n_masks, 512, 512))\n",
    "        # load the data from img_lst\n",
    "        for index, i in enumerate(img_list):\n",
    "            loc = os.path.join(dir, i)\n",
    "            img = image.imread(loc)\n",
    "            if re.match(pattern, i):\n",
    "                mask = img\n",
    "                mask = (mask >= 128).astype(int)\n",
    "                mask = 4 * mask[:, :, 0] + 2 * mask[:, :, 1] + mask[:, :, 2]\n",
    "                masks[index // 2, mask == 3] = 0  # (Cyan: 011) Urban land \n",
    "                masks[index // 2, mask == 6] = 1  # (Yellow: 110) Agriculture land \n",
    "                masks[index // 2, mask == 5] = 2  # (Purple: 101) Rangeland \n",
    "                masks[index // 2, mask == 2] = 3  # (Green: 010) Forest land \n",
    "                masks[index // 2, mask == 1] = 4  # (Blue: 001) Water \n",
    "                masks[index // 2, mask == 7] = 5  # (White: 111) Barren land \n",
    "                masks[index // 2, mask == 0] = 6  # (Black: 000) Unknown \n",
    "            else:\n",
    "                imgs.append(img)\n",
    "        self.imgs = np.array(imgs, dtype = np.float32)[:,::2,::2, :]\n",
    "        self.lbls = masks[:,::2,::2]\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        lbl = self.lbls[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, lbl\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a798a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose( # composing several transforms together\n",
    "    [transforms.ToTensor(), # to tensor object\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "datadir = \"./hw1_data/p2_data/train\"\n",
    "test_dataset = SegData(datadir, transform)\n",
    "valdir = \"./hw1_data/p2_data/validation\"\n",
    "val_dataset = SegData(valdir, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b197d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_classes = 7\n",
    "momentum = 0\n",
    "w_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78441692",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else \"cpu\"\n",
    "train_dl = DataLoader(test_dataset, batch_size, shuffle = True)\n",
    "val_dl = DataLoader(val_dataset, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81da059",
   "metadata": {},
   "outputs": [],
   "source": [
    "test, lab = next(iter(val_dl))\n",
    "# print(lab[0].shape)\n",
    "print(lab[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1e94a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained= True)\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "037e15d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCN32, self).__init__()\n",
    "        self.network = nn.Sequential(*(list(vgg16.children())[:-1]))\n",
    "        self.convs =nn.Sequential(nn.Conv2d(512,4096,3),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout(),\n",
    "                                nn.Conv2d(4096,4096,1),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout(),\n",
    "                                nn.Conv2d(4096,64,1),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout()\n",
    "                                )\n",
    "        self.upscore=nn.ConvTranspose2d(64,num_classes,80,108)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_size=x.size()\n",
    "        x = self.network(x)\n",
    "        pool=self.convs(x)\n",
    "        upscore=self.upscore(pool)\n",
    "        return upscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd19ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN32().to(device)\n",
    "\n",
    "lr = 2e-4 # 1e-3 behaves better\n",
    "epochs = 40\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee9ad0ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "torch.Size([50, 7, 512, 512])\n",
      "torch.Size([50, 512, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"log_softmax\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/haotinghsieh/Desktop/Vision & Learning Lab/hw1/hw1_p2.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haotinghsieh/Desktop/Vision%20%26%20Learning%20Lab/hw1/hw1_p2.ipynb#ch0000015?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haotinghsieh/Desktop/Vision%20%26%20Learning%20Lab/hw1/hw1_p2.ipynb#ch0000015?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/haotinghsieh/Desktop/Vision%20%26%20Learning%20Lab/hw1/hw1_p2.ipynb#ch0000015?line=24'>25</a>\u001b[0m     train_model(train_dl, model, loss_fn, optimizer)\n",
      "\u001b[1;32m/Users/haotinghsieh/Desktop/Vision & Learning Lab/hw1/hw1_p2.ipynb Cell 14'\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haotinghsieh/Desktop/Vision%20%26%20Learning%20Lab/hw1/hw1_p2.ipynb#ch0000015?line=11'>12</a>\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(pred, dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haotinghsieh/Desktop/Vision%20%26%20Learning%20Lab/hw1/hw1_p2.ipynb#ch0000015?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(pred\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/haotinghsieh/Desktop/Vision%20%26%20Learning%20Lab/hw1/hw1_p2.ipynb#ch0000015?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haotinghsieh/Desktop/Vision%20%26%20Learning%20Lab/hw1/hw1_p2.ipynb#ch0000015?line=14'>15</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haotinghsieh/Desktop/Vision%20%26%20Learning%20Lab/hw1/hw1_p2.ipynb#ch0000015?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1161'>1162</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1162'>1163</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1163'>1164</a>\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1164'>1165</a>\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/functional.py?line=2993'>2994</a>\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/functional.py?line=2994'>2995</a>\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> <a href='file:///Users/haotinghsieh/opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/nn/functional.py?line=2995'>2996</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "def train_model(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (data, label) in enumerate(dataloader, 0):\n",
    "        # Compute prediction and loss\n",
    "        model.zero_grad()\n",
    "        label = Variable(label.type(torch.FloatTensor))    \n",
    "        X = data.to(device)\n",
    "        y = label.to(device)\n",
    "        pred = model(X)\n",
    "        pred = F.log_softmax(pred, dim= 1)\n",
    "        print(pred.shape)\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "        print(pred.shape)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 10 == 0:\n",
    "            # print out result every 100 mini batches\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}]\")\n",
    "            \n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_model(train_dl, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea44ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"hw1p2_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader):\n",
    "    model.eval()     \n",
    "    pred = torch.FloatTensor()\n",
    "    pred = pred.cuda()\n",
    "    for batch, data in enumerate(testloader, 0):\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        images = images.view(1,3,256,256)\n",
    "        output = model(images)\n",
    "        pred = torch.cat((pred,output.data),0)\n",
    "        \n",
    "    pred = np.argmax(pred,1) \n",
    "    pred_512 = np.array([resize(p,output_shape=(512,512), order=0,preserve_range=True,clip=True) for p in pred])\n",
    "    mean_iou = mean_iou_score(pred_512, valid_y)\n",
    "    print(\"mean iou score\", mean_iou)  \n",
    "\n",
    "test(model, val_dl)\n",
    "# save the pred map from the mask\n",
    "#     if epoch+1 in [1,10,20]: # save pred map\n",
    "#         # decoding stage\n",
    "#         n_masks = len(valid_X)\n",
    "#         masks_RGB = np.empty((n_masks, 512, 512, 3))\n",
    "#         for i, p in enumerate(pred_512):\n",
    "#             masks_RGB[i, p == 0] = [0,255,255]\n",
    "#             masks_RGB[i, p == 1] = [255,255,0]\n",
    "#             masks_RGB[i, p == 2] = [255,0,255]\n",
    "#             masks_RGB[i, p == 3] = [0,255,0]\n",
    "#             masks_RGB[i, p == 4] = [0,0,255]\n",
    "#             masks_RGB[i, p == 5] = [255,255,255]\n",
    "#             masks_RGB[i, p == 6] = [0,0,0]\n",
    "#         masks_RGB = masks_RGB.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11608912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strides = 108\n",
    "kernel_size = 80\n",
    "padding = 0\n",
    "input_size = 5\n",
    "output_size = strides * (input_size-1) + kernel_size - 2*padding\n",
    "print(output_size)\n",
    "input_size = 512\n",
    "output_size = ((input_size + (padding * 2) - kernel_size) / strides) + 1\n",
    "print(output_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
